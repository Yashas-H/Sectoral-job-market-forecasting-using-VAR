{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw-ek4uefPgU",
        "outputId": "1759da6d-06b6-445f-a673-b0db37274d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting fredapi\n",
            "  Downloading fredapi-0.5.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading fredapi-0.5.2-py3-none-any.whl (11 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule, fredapi\n",
            "Successfully installed fredapi-0.5.2 schedule-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests fredapi pandas schedule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "from scipy import interpolate\n",
        "from statsmodels import robust\n",
        "\n",
        "# BLS API configuration\n",
        "BLS_API_KEY = \"58d8a969b3424879b55320f337696d8a\"  # Replace with your BLS API key\n",
        "BLS_BASE_URL = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\n",
        "\n",
        "# JOLTS series IDs with human-readable names\n",
        "jolt_series = {\n",
        "    \"JTS000000000000000JOL\": \"Total_Nonfarm_Job_Openings\",\n",
        "    \"JTS300000000000000JOL\": \"Manufacturing_Job_Openings\",\n",
        "    \"JTS600000000000000JOL\": \"Professional_Services_Job_Openings\",\n",
        "    \"JTS700000000000000JOL\": \"Leisure_Hospitality_Job_Openings\",\n",
        "}\n",
        "\n",
        "# def clean_series(df, column='value'):\n",
        "#     \"\"\"\n",
        "#     Clean series by handling missing values, non-numeric values, and outliers.\n",
        "#     \"\"\"\n",
        "#     df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "#     df[column] = df[column].ffill().bfill()\n",
        "\n",
        "#     Q1 = df[column].quantile(0.25)\n",
        "#     Q3 = df[column].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "#     df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "#     if df[column].isna().any():\n",
        "#         print(f\"Warning: {column} has {df[column].isna().sum()} NaN values after cleaning\")\n",
        "\n",
        "#     return df\n",
        "\n",
        "def interpolate_to_weekly(df, date_col='date', value_col='value', method='linear'):\n",
        "    \"\"\"\n",
        "    Interpolate series to weekly frequency.\n",
        "    \"\"\"\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    df_clean = df.dropna(subset=[value_col]).copy()\n",
        "\n",
        "    if df_clean.empty:\n",
        "        print(f\"Warning: No valid data for interpolation\")\n",
        "        return None\n",
        "\n",
        "    start_date = df_clean[date_col].min()\n",
        "    end_date = df_clean[date_col].max()\n",
        "    weekly_dates = pd.date_range(start=start_date, end=end_date, freq='W-MON')\n",
        "\n",
        "    df_clean.loc[:, 'numeric_date'] = (df_clean[date_col] - df_clean[date_col].min()).dt.days\n",
        "\n",
        "    try:\n",
        "        if method == 'cubic':\n",
        "            interp_func = interpolate.interp1d(df_clean['numeric_date'], df_clean[value_col], kind='cubic', fill_value=\"extrapolate\")\n",
        "            interpolated_values = interp_func((weekly_dates - start_date).days)\n",
        "        else:  # linear\n",
        "            interp_func = interpolate.interp1d(df_clean['numeric_date'], df_clean[value_col], kind='linear', fill_value=\"extrapolate\")\n",
        "            interpolated_values = interp_func((weekly_dates - start_date).days)\n",
        "\n",
        "        interpolated_df = pd.DataFrame({\n",
        "            date_col: weekly_dates,\n",
        "            value_col: interpolated_values\n",
        "        })\n",
        "        return interpolated_df\n",
        "    except Exception as e:\n",
        "        print(f\"Interpolation failed for method {method}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_bls_data(series_ids, start_year=2000, end_year=datetime.now().year):\n",
        "    \"\"\"\n",
        "    Fetch BLS JOLTS data in decade-long batches, interpolate to weekly, and combine into a single DataFrame.\n",
        "    Returns a pandas DataFrame with weekly data and human-readable columns.\n",
        "    \"\"\"\n",
        "    headers = {\"Content-type\": \"application/json\"}\n",
        "    all_combined_dfs = []\n",
        "\n",
        "    # Define decade ranges\n",
        "    current_year = min(end_year, datetime.now().year)\n",
        "    year_ranges = []\n",
        "    year = start_year\n",
        "    while year <= current_year:\n",
        "        decade_start = year\n",
        "        decade_end = min(year + 9, current_year)\n",
        "        year_ranges.append((decade_start, decade_end))\n",
        "        year = decade_end + 1\n",
        "\n",
        "    for start_y, end_y in year_ranges:\n",
        "        print(f\"Fetching BLS JOLTS data for {start_y}-{end_y}...\")\n",
        "        payload = {\n",
        "            \"seriesid\": list(series_ids.keys()),\n",
        "            \"startyear\": str(start_y),\n",
        "            \"endyear\": str(end_y),\n",
        "            \"registrationkey\": BLS_API_KEY\n",
        "        }\n",
        "\n",
        "        weekly_dfs = []\n",
        "\n",
        "        try:\n",
        "            # Fetch data from BLS API\n",
        "            response = requests.post(BLS_BASE_URL, json=payload, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if data[\"status\"] != \"REQUEST_SUCCEEDED\":\n",
        "                raise Exception(f\"API error for {start_y}-{end_y}: {data['message']}\")\n",
        "\n",
        "            # Process each series\n",
        "            for series in data[\"Results\"][\"series\"]:\n",
        "                series_id = series[\"seriesID\"]\n",
        "                series_name = jolt_series[series_id]\n",
        "                print(f\"Processing {series_id} ({series_name}) for {start_y}-{end_y}...\")\n",
        "\n",
        "                # Extract data points\n",
        "                results = []\n",
        "                for item in series[\"data\"]:\n",
        "                    try:\n",
        "                        results.append({\n",
        "                            \"series_id\": series_id,\n",
        "                            \"sector\": series_name,\n",
        "                            \"date\": f\"{item['year']}-{item['period'][1:]}-01\",\n",
        "                            \"value\": float(item[\"value\"])\n",
        "                        })\n",
        "                    except (ValueError, KeyError) as e:\n",
        "                        print(f\"Warning: Skipping invalid data point for {series_id}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                if not results:\n",
        "                    print(f\"Warning: No valid data for {series_id} ({series_name}) in {start_y}-{end_y}\")\n",
        "                    continue\n",
        "\n",
        "                df = pd.DataFrame(results)\n",
        "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "                # Check and remove duplicates\n",
        "                duplicates = df[df.duplicated(subset=['date', 'series_id'], keep=False)]\n",
        "                if len(duplicates) > 0:\n",
        "                    print(f\"Warning: {series_id} ({series_name}) has {len(duplicates)} duplicates in {start_y}-{end_y}\")\n",
        "                    df = df.groupby(['date', 'series_id']).agg({'value': 'mean', 'sector': 'first'}).reset_index()\n",
        "\n",
        "                # Interpolate to weekly\n",
        "                print(f\"Interpolating {series_name} to weekly for {start_y}-{end_y}...\")\n",
        "                interpolated_df = interpolate_to_weekly(df, method='linear')\n",
        "                if interpolated_df is not None:\n",
        "                    #interpolated_df = clean_series(interpolated_df)\n",
        "                    weekly_dfs.append(interpolated_df.rename(columns={\"value\": series_name}))\n",
        "\n",
        "            # Combine weekly DataFrames for this decade\n",
        "            if weekly_dfs:\n",
        "                combined_df = weekly_dfs[0]\n",
        "                for df in weekly_dfs[1:]:\n",
        "                    combined_df = combined_df.merge(df, on='date', how='outer')\n",
        "                all_combined_dfs.append(combined_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching BLS data for {start_y}-{end_y}: {e}\")\n",
        "            print(\"Check API key at https://www.bls.gov/developers/api_faqs.htm or data availability.\")\n",
        "            continue\n",
        "\n",
        "    # Combine all decade DataFrames\n",
        "    if all_combined_dfs:\n",
        "        final_df = pd.concat(all_combined_dfs, axis=0, ignore_index=True)\n",
        "        final_df = final_df.groupby('date').first().reset_index()  # Remove any overlapping dates\n",
        "        final_df = final_df.set_index('date').sort_index()\n",
        "\n",
        "        # Fill missing values\n",
        "        final_df = final_df.ffill().bfill()\n",
        "\n",
        "        # Remove any duplicate dates\n",
        "        final_df = final_df.loc[~final_df.index.duplicated(keep='last')]\n",
        "\n",
        "        # Verify columns\n",
        "        expected_columns = list(jolt_series.values())\n",
        "        missing_columns = [col for col in expected_columns if col not in final_df.columns]\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "\n",
        "        print(\"BLS JOLTS data processed successfully\")\n",
        "        return final_df.reset_index()\n",
        "    else:\n",
        "        print(\"No BLS JOLTS data processed\")\n",
        "        return None\n",
        "\n",
        "bls_data = fetch_bls_data(jolt_series, start_year=2000)\n",
        "if bls_data is not None:\n",
        "    print(\"BLS JOLTS Weekly Data Sample:\")\n",
        "    print(bls_data.head())\n",
        "    print(f\"Columns: {bls_data.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rGo7mm1iPxe",
        "outputId": "1a999571-86e4-4804-f135-895d573bd101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching BLS JOLTS data for 2000-2009...\n",
            "Processing JTS000000000000000JOL (Total_Nonfarm_Job_Openings) for 2000-2009...\n",
            "Interpolating Total_Nonfarm_Job_Openings to weekly for 2000-2009...\n",
            "Processing JTS300000000000000JOL (Manufacturing_Job_Openings) for 2000-2009...\n",
            "Interpolating Manufacturing_Job_Openings to weekly for 2000-2009...\n",
            "Processing JTS600000000000000JOL (Professional_Services_Job_Openings) for 2000-2009...\n",
            "Interpolating Professional_Services_Job_Openings to weekly for 2000-2009...\n",
            "Processing JTS700000000000000JOL (Leisure_Hospitality_Job_Openings) for 2000-2009...\n",
            "Interpolating Leisure_Hospitality_Job_Openings to weekly for 2000-2009...\n",
            "Fetching BLS JOLTS data for 2010-2019...\n",
            "Processing JTS000000000000000JOL (Total_Nonfarm_Job_Openings) for 2010-2019...\n",
            "Interpolating Total_Nonfarm_Job_Openings to weekly for 2010-2019...\n",
            "Processing JTS300000000000000JOL (Manufacturing_Job_Openings) for 2010-2019...\n",
            "Interpolating Manufacturing_Job_Openings to weekly for 2010-2019...\n",
            "Processing JTS600000000000000JOL (Professional_Services_Job_Openings) for 2010-2019...\n",
            "Interpolating Professional_Services_Job_Openings to weekly for 2010-2019...\n",
            "Processing JTS700000000000000JOL (Leisure_Hospitality_Job_Openings) for 2010-2019...\n",
            "Interpolating Leisure_Hospitality_Job_Openings to weekly for 2010-2019...\n",
            "Fetching BLS JOLTS data for 2020-2025...\n",
            "Processing JTS000000000000000JOL (Total_Nonfarm_Job_Openings) for 2020-2025...\n",
            "Interpolating Total_Nonfarm_Job_Openings to weekly for 2020-2025...\n",
            "Processing JTS300000000000000JOL (Manufacturing_Job_Openings) for 2020-2025...\n",
            "Interpolating Manufacturing_Job_Openings to weekly for 2020-2025...\n",
            "Processing JTS600000000000000JOL (Professional_Services_Job_Openings) for 2020-2025...\n",
            "Interpolating Professional_Services_Job_Openings to weekly for 2020-2025...\n",
            "Processing JTS700000000000000JOL (Leisure_Hospitality_Job_Openings) for 2020-2025...\n",
            "Interpolating Leisure_Hospitality_Job_Openings to weekly for 2020-2025...\n",
            "BLS JOLTS data processed successfully\n",
            "BLS JOLTS Weekly Data Sample:\n",
            "        date  Total_Nonfarm_Job_Openings  Manufacturing_Job_Openings  \\\n",
            "0 2000-12-04                 5102.129032                  426.935484   \n",
            "1 2000-12-11                 5135.096774                  431.451613   \n",
            "2 2000-12-18                 5168.064516                  435.967742   \n",
            "3 2000-12-25                 5201.032258                  440.483871   \n",
            "4 2001-01-01                 5234.000000                  445.000000   \n",
            "\n",
            "   Professional_Services_Job_Openings  Leisure_Hospitality_Job_Openings  \n",
            "0                          911.548387                        574.516129  \n",
            "1                          894.161290                        594.387097  \n",
            "2                          876.774194                        614.258065  \n",
            "3                          859.387097                        634.129032  \n",
            "4                          842.000000                        654.000000  \n",
            "Columns: ['date', 'Total_Nonfarm_Job_Openings', 'Manufacturing_Job_Openings', 'Professional_Services_Job_Openings', 'Leisure_Hospitality_Job_Openings']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fredapi import Fred\n",
        "from datetime import datetime\n",
        "from scipy import interpolate\n",
        "from statsmodels import robust\n",
        "\n",
        "# FRED API configuration\n",
        "FRED_API_KEY = \"8fc0d8bd4ea5422e403a182588e4920a\"  # Replace with your FRED API key\n",
        "fred = Fred(api_key=FRED_API_KEY)\n",
        "\n",
        "# FRED series IDs with human-readable names and frequencies\n",
        "fred_series = {\n",
        "\n",
        "    \"ICSA\": {\"name\": \"Initial_Jobless_Claims\", \"frequency\": \"weekly\"},\n",
        "    \"PAYEMS\": {\"name\": \"Total_Nonfarm_Employment\", \"frequency\": \"monthly\"},\n",
        "    \"UNRATE\": {\"name\": \"Unemployment_Rate\", \"frequency\": \"monthly\"},\n",
        "    \"MANEMP\": {\"name\": \"Manufacturing_Employment\", \"frequency\": \"monthly\"},\n",
        "    \"USPBS\": {\"name\": \"Professional_Services_Employment\", \"frequency\": \"monthly\"},\n",
        "    \"USLAH\": {\"name\": \"Leisure_Hospitality_Employment\", \"frequency\": \"monthly\"},\n",
        "    \"CES0500000003\": {\"name\": \"Average_Hourly_Earnings\", \"frequency\": \"monthly\"},\n",
        "    \"AWHAETP\": {\"name\": \"Weekly_Hours_Worked\", \"frequency\": \"monthly\"},\n",
        "    \"GDPC1\": {\"name\": \"GDP\", \"frequency\": \"quarterly\"},\n",
        "    \"CSUSHPINSA\": {\"name\": \"Housing_Prices\", \"frequency\": \"monthly\"},\n",
        "    \"FEDFUNDS\": {\"name\": \"Federal_Funds_Rate\", \"frequency\": \"monthly\"}\n",
        "}\n",
        "\n",
        "def clean_series(df, column='value'):\n",
        "    \"\"\"\n",
        "    Clean series by handling missing values, non-numeric values, and outliers.\n",
        "    \"\"\"\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    df[column] = df[column].ffill().bfill()\n",
        "\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    if df[column].isna().any():\n",
        "        print(f\"Warning: {column} has {df[column].isna().sum()} NaN values after cleaning\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def interpolate_to_weekly(df, date_col='date', value_col='value', method='linear'):\n",
        "    \"\"\"\n",
        "    Interpolate series to weekly frequency.\n",
        "    \"\"\"\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    df_clean = df.dropna(subset=[value_col]).copy()\n",
        "\n",
        "    if df_clean.empty:\n",
        "        print(f\"Warning: No valid data for interpolation\")\n",
        "        return None\n",
        "\n",
        "    start_date = df_clean[date_col].min()\n",
        "    end_date = df_clean[date_col].max()\n",
        "    weekly_dates = pd.date_range(start=start_date, end=end_date, freq='W-MON')\n",
        "\n",
        "    df_clean.loc[:, 'numeric_date'] = (df_clean[date_col] - df_clean[date_col].min()).dt.days\n",
        "\n",
        "    try:\n",
        "        if method == 'cubic':\n",
        "            interp_func = interpolate.interp1d(df_clean['numeric_date'], df_clean[value_col], kind='cubic', fill_value=\"extrapolate\")\n",
        "            interpolated_values = interp_func((weekly_dates - start_date).days)\n",
        "        else:  # linear\n",
        "            interp_func = interpolate.interp1d(df_clean['numeric_date'], df_clean[value_col], kind='linear', fill_value=\"extrapolate\")\n",
        "            interpolated_values = interp_func((weekly_dates - start_date).days)\n",
        "\n",
        "        interpolated_df = pd.DataFrame({\n",
        "            date_col: weekly_dates,\n",
        "            value_col: interpolated_values\n",
        "        })\n",
        "        return interpolated_df\n",
        "    except Exception as e:\n",
        "        print(f\"Interpolation failed for method {method}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_fred_data(series_ids, start_year=2000, end_year=datetime.now().year):\n",
        "    \"\"\"\n",
        "    Fetch FRED data, separate by frequency, interpolate to weekly, and combine into a single DataFrame.\n",
        "    Ensures SP500 is included as a separate column and prevents sector column issues.\n",
        "    Returns a pandas DataFrame with weekly data and human-readable columns.\n",
        "    \"\"\"\n",
        "    daily_dfs = []\n",
        "    non_daily_dfs = []\n",
        "\n",
        "    # Fetch and categorize series by frequency\n",
        "    for series_id, info in series_ids.items():\n",
        "        series_name = info[\"name\"]\n",
        "        frequency = info[\"frequency\"]\n",
        "\n",
        "        try:\n",
        "            # Fetch series data with debugging\n",
        "            print(f\"Fetching {series_id} ({series_name})...\")\n",
        "            data = fred.get_series(series_id, observation_start=f\"{start_year}-01-01\", observation_end=f\"{end_year}-12-31\")\n",
        "\n",
        "            if data is None or data.empty:\n",
        "                print(f\"Warning: No data returned for {series_id} ({series_name})\")\n",
        "                if series_id == \"SP500\":\n",
        "                    print(\"SP500 fetch failed. Check API key permissions at https://fredaccount.stlouisfed.org/apikeys or data availability.\")\n",
        "                continue\n",
        "\n",
        "            # Log data summary\n",
        "            print(f\"Data for {series_id}: {len(data)} rows, first few values: {data.head().to_list()}\")\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            df = data.reset_index().rename(columns={\"index\": \"date\", 0: \"value\"})\n",
        "            df[\"series_id\"] = series_id\n",
        "            df[\"sector\"] = series_name\n",
        "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "            df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "\n",
        "            # Check for valid data\n",
        "            valid_count = df[\"value\"].notna().sum()\n",
        "            if valid_count < len(df) * 0.1:  # Allow partial data\n",
        "                print(f\"Warning: {series_id} ({series_name}) has only {valid_count} valid values out of {len(df)}\")\n",
        "                continue\n",
        "\n",
        "            # Check and remove duplicates\n",
        "            duplicates = df[df.duplicated(subset=['date', 'series_id'], keep=False)]\n",
        "            if len(duplicates) > 0:\n",
        "                print(f\"Warning: {series_id} ({series_name}) has {len(duplicates)} duplicates\")\n",
        "                print(duplicates.head())\n",
        "                df = df.groupby(['date', 'series_id']).agg({'value': 'mean', 'sector': 'first'}).reset_index()\n",
        "\n",
        "            # Categorize by frequency\n",
        "            if frequency == \"daily\":\n",
        "                daily_dfs.append(df)\n",
        "            else:\n",
        "                non_daily_dfs.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {series_id} ({series_name}): {e}\")\n",
        "            if series_id == \"SP500\":\n",
        "                print(\"SP500 fetch failed. Check API key permissions at https://fredaccount.stlouisfed.org/apikeys or data availability.\")\n",
        "            continue\n",
        "\n",
        "    # Process daily data (aggregate to weekly)\n",
        "    weekly_dfs = []\n",
        "    for df in daily_dfs:\n",
        "        series_name = df[\"sector\"].iloc[0]\n",
        "        print(f\"Aggregating daily data for {series_name} to weekly...\")\n",
        "\n",
        "        # Fill missing days (e.g., weekends for SP500)\n",
        "        df = df.set_index('date').resample('D').ffill().reset_index()\n",
        "        df = df.resample('W-MON', on='date').mean(numeric_only=True).reset_index()\n",
        "\n",
        "        if df.empty or df[\"value\"].isna().all():\n",
        "            print(f\"Warning: No valid weekly data for {series_name} after aggregation\")\n",
        "            if series_name == \"SP500\":\n",
        "                print(\"SP500 aggregation failed. Verify data availability or API response.\")\n",
        "            continue\n",
        "\n",
        "        df = clean_series(df)\n",
        "        # Rename to ensure SP500 is a data column, not sector\n",
        "        weekly_dfs.append(df[['date', 'value']].rename(columns={\"value\": series_name}))\n",
        "\n",
        "    # Process non-daily data (interpolate to weekly)\n",
        "    for df in non_daily_dfs:\n",
        "        series_name = df[\"sector\"].iloc[0]\n",
        "        frequency = [info[\"frequency\"] for series_id, info in series_ids.items() if info[\"name\"] == series_name][0]\n",
        "\n",
        "        # Choose interpolation method\n",
        "        method = 'cubic' if frequency == \"quarterly\" else 'linear'\n",
        "\n",
        "        print(f\"Interpolating {series_name} ({frequency}) to weekly with {method} method...\")\n",
        "        interpolated_df = interpolate_to_weekly(df, method=method)\n",
        "        if interpolated_df is not None:\n",
        "            interpolated_df = clean_series(interpolated_df)\n",
        "            # Rename to ensure data column, not sector\n",
        "            weekly_dfs.append(interpolated_df[['date', 'value']].rename(columns={\"value\": series_name}))\n",
        "\n",
        "    # Combine all weekly DataFrames\n",
        "    if weekly_dfs:\n",
        "        # Merge on date, preserving all dates\n",
        "        combined_df = weekly_dfs[0]\n",
        "        for df in weekly_dfs[1:]:\n",
        "            combined_df = combined_df.merge(df, on='date', how='outer')\n",
        "\n",
        "        # Set date as index and fill missing values\n",
        "        combined_df = combined_df.set_index('date').sort_index()\n",
        "        combined_df = combined_df.ffill().bfill()\n",
        "\n",
        "        # Remove any duplicate dates\n",
        "        combined_df = combined_df.loc[~combined_df.index.duplicated(keep='last')]\n",
        "\n",
        "        # Verify columns, especially SP500\n",
        "        expected_columns = [info[\"name\"] for series_id, info in series_ids.items()]\n",
        "        missing_columns = [col for col in expected_columns if col not in combined_df.columns]\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            if \"SP500\" in missing_columns:\n",
        "                print(\"SP500 missing in final DataFrame. Verify API key at https://fredaccount.stlouisfed.org/apikeys or data availability.\")\n",
        "\n",
        "        print(\"FRED data processed successfully\")\n",
        "        return combined_df.reset_index()\n",
        "    else:\n",
        "        print(\"No FRED data processed\")\n",
        "        return None\n",
        "\n",
        "fred_data = fetch_fred_data(fred_series, start_year=2000)\n",
        "if fred_data is not None:\n",
        "    print(\"FRED Weekly Data Sample:\")\n",
        "    print(fred_data.head())\n",
        "    print(f\"Columns: {fred_data.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoWEvbp9gZqN",
        "outputId": "b6c230f3-2d15-431b-9a79-d758fc49f50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching ICSA (Initial_Jobless_Claims)...\n",
            "Data for ICSA: 1321 rows, first few values: [286000.0, 298000.0, 289000.0, 284000.0, 285000.0]\n",
            "Fetching PAYEMS (Total_Nonfarm_Employment)...\n",
            "Data for PAYEMS: 303 rows, first few values: [131011.0, 131121.0, 131604.0, 131883.0, 132105.0]\n",
            "Fetching UNRATE (Unemployment_Rate)...\n",
            "Data for UNRATE: 303 rows, first few values: [4.0, 4.1, 4.0, 3.8, 4.0]\n",
            "Fetching MANEMP (Manufacturing_Employment)...\n",
            "Data for MANEMP: 303 rows, first few values: [17284.0, 17284.0, 17302.0, 17299.0, 17278.0]\n",
            "Fetching USPBS (Professional_Services_Employment)...\n",
            "Data for USPBS: 303 rows, first few values: [16488.0, 16525.0, 16599.0, 16682.0, 16647.0]\n",
            "Fetching USLAH (Leisure_Hospitality_Employment)...\n",
            "Data for USLAH: 303 rows, first few values: [11713.0, 11719.0, 11788.0, 11834.0, 11827.0]\n",
            "Fetching CES0500000003 (Average_Hourly_Earnings)...\n",
            "Data for CES0500000003: 229 rows, first few values: [20.05, 20.15, 20.13, 20.23, 20.29]\n",
            "Fetching AWHAETP (Weekly_Hours_Worked)...\n",
            "Data for AWHAETP: 229 rows, first few values: [34.2, 34.3, 34.3, 34.4, 34.3]\n",
            "Fetching GDPC1 (GDP)...\n",
            "Data for GDPC1: 101 rows, first few values: [13878.147, 14130.908, 14145.312, 14229.765, 14183.12]\n",
            "Fetching CSUSHPINSA (Housing_Prices)...\n",
            "Data for CSUSHPINSA: 302 rows, first few values: [100.0, 100.571, 101.46600000000001, 102.541, 103.70200000000001]\n",
            "Fetching FEDFUNDS (Federal_Funds_Rate)...\n",
            "Data for FEDFUNDS: 303 rows, first few values: [5.45, 5.73, 5.85, 6.02, 6.27]\n",
            "Interpolating Initial_Jobless_Claims (weekly) to weekly with linear method...\n",
            "Interpolating Total_Nonfarm_Employment (monthly) to weekly with linear method...\n",
            "Interpolating Unemployment_Rate (monthly) to weekly with linear method...\n",
            "Interpolating Manufacturing_Employment (monthly) to weekly with linear method...\n",
            "Interpolating Professional_Services_Employment (monthly) to weekly with linear method...\n",
            "Interpolating Leisure_Hospitality_Employment (monthly) to weekly with linear method...\n",
            "Interpolating Average_Hourly_Earnings (monthly) to weekly with linear method...\n",
            "Interpolating Weekly_Hours_Worked (monthly) to weekly with linear method...\n",
            "Interpolating GDP (quarterly) to weekly with cubic method...\n",
            "Interpolating Housing_Prices (monthly) to weekly with linear method...\n",
            "Interpolating Federal_Funds_Rate (monthly) to weekly with linear method...\n",
            "FRED data processed successfully\n",
            "FRED Weekly Data Sample:\n",
            "        date  Initial_Jobless_Claims  Total_Nonfarm_Employment  \\\n",
            "0 2000-01-03           289428.571429             131018.096774   \n",
            "1 2000-01-10           295428.571429             131042.935484   \n",
            "2 2000-01-17           287571.428571             131067.774194   \n",
            "3 2000-01-24           284285.714286             131092.612903   \n",
            "4 2000-01-31           292714.285714             131117.451613   \n",
            "\n",
            "   Unemployment_Rate  Manufacturing_Employment  \\\n",
            "0           4.006452               17196.33871   \n",
            "1           4.029032               17196.33871   \n",
            "2           4.051613               17196.33871   \n",
            "3           4.074194               17196.33871   \n",
            "4           4.096774               17196.33871   \n",
            "\n",
            "   Professional_Services_Employment  Leisure_Hospitality_Employment  \\\n",
            "0                      16490.387097                    11713.387097   \n",
            "1                      16498.741935                    11714.741935   \n",
            "2                      16507.096774                    11716.096774   \n",
            "3                      16515.451613                    11717.451613   \n",
            "4                      16523.806452                    11718.806452   \n",
            "\n",
            "   Average_Hourly_Earnings  Weekly_Hours_Worked           GDP  Housing_Prices  \\\n",
            "0                20.066129            34.216129  13889.729053      100.036839   \n",
            "1                20.066129            34.216129  13927.574431      100.165774   \n",
            "2                20.066129            34.216129  13961.402132      100.294710   \n",
            "3                20.066129            34.216129  13991.434730      100.423645   \n",
            "4                20.066129            34.216129  14017.894797      100.552581   \n",
            "\n",
            "   Federal_Funds_Rate  \n",
            "0            5.468065  \n",
            "1            5.531290  \n",
            "2            5.594516  \n",
            "3            5.657742  \n",
            "4            5.720968  \n",
            "Columns: ['date', 'Initial_Jobless_Claims', 'Total_Nonfarm_Employment', 'Unemployment_Rate', 'Manufacturing_Employment', 'Professional_Services_Employment', 'Leisure_Hospitality_Employment', 'Average_Hourly_Earnings', 'Weekly_Hours_Worked', 'GDP', 'Housing_Prices', 'Federal_Funds_Rate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from statsmodels import robust\n",
        "\n",
        "\n",
        "# def clean_series(df, column, fill_missing=True):\n",
        "#     \"\"\"\n",
        "#     Clean series by handling non-numeric values and outliers.\n",
        "#     Optionally skip filling missing values for BLS columns.\n",
        "#     \"\"\"\n",
        "#     df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "\n",
        "#     if fill_missing:\n",
        "#         df[column] = df[column].ffill().bfill()\n",
        "\n",
        "#     Q1 = df[column].quantile(0.25)\n",
        "#     Q3 = df[column].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "#     df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "#     if df[column].isna().any():\n",
        "#         print(f\"Warning: {column} has {df[column].isna().sum()} NaN values after cleaning\")\n",
        "\n",
        "#     return df\n",
        "\n",
        "def preprocess_data(start_year=2000, end_year=datetime.now().year):\n",
        "    \"\"\"\n",
        "    Fetch FRED and BLS data, combine into a single weekly DataFrame starting from the first overlapping date,\n",
        "    and produce df_2000 (from overlap to present) and df_2006 (2006–present).\n",
        "    Uses inner merge and avoids filling missing BLS data.\n",
        "    Returns two DataFrames: df_2000, df_2006.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if fred_data is None or bls_data is None:\n",
        "        print(\"Error: Failed to fetch FRED or BLS data\")\n",
        "        return None, None\n",
        "\n",
        "    # Ensure date is datetime\n",
        "    fred_data['date'] = pd.to_datetime(fred_data['date'])\n",
        "    bls_data['date'] = pd.to_datetime(bls_data['date'])\n",
        "\n",
        "    # Debugging: Data summary\n",
        "    print(\"\\nFRED Data Summary:\")\n",
        "    print(f\"Shape: {fred_data.shape}\")\n",
        "    print(f\"Columns: {fred_data.columns.tolist()}\")\n",
        "    print(f\"Date Range: {fred_data['date'].min()} to {fred_data['date'].max()}\")\n",
        "\n",
        "    print(\"\\nBLS Data Summary:\")\n",
        "    print(f\"Shape: {bls_data.shape}\")\n",
        "    print(f\"Columns: {bls_data.columns.tolist()}\")\n",
        "    print(f\"Date Range: {bls_data['date'].min()} to {bls_data['date'].max()}\")\n",
        "\n",
        "    # Check for duplicate dates\n",
        "    for df, name in [(fred_data, \"FRED\"), (bls_data, \"BLS\")]:\n",
        "        duplicates = df[df['date'].duplicated(keep=False)]\n",
        "        if len(duplicates) > 0:\n",
        "            print(f\"Warning: {name} data has {len(duplicates)} duplicate dates\")\n",
        "            print(duplicates.head())\n",
        "            df = df.groupby('date').mean(numeric_only=True).reset_index()\n",
        "\n",
        "    # Merge FRED and BLS data using inner merge\n",
        "    print(\"\\nMerging FRED and BLS data from first overlapping date...\")\n",
        "    combined_df = pd.merge(fred_data, bls_data, on='date', how='inner')\n",
        "    combined_df = combined_df.sort_values('date').set_index('date')\n",
        "\n",
        "    # Clean columns, filling missing values only for FRED columns\n",
        "    fred_columns = [info[\"name\"] for series_id, info in fred_series.items()]\n",
        "    bls_columns = list(jolt_series.values())\n",
        "\n",
        "    for column in combined_df.columns:\n",
        "        fill_missing = column in fred_columns  # Only fill for FRED columns\n",
        "        # combined_df = clean_series(combined_df, column, fill_missing=fill_missing)\n",
        "\n",
        "    # Remove any duplicate dates\n",
        "    combined_df = combined_df.loc[~combined_df.index.duplicated(keep='last')]\n",
        "\n",
        "    # Verify columns\n",
        "    expected_columns = fred_columns + bls_columns\n",
        "    missing_columns = [col for col in expected_columns if col not in combined_df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: Missing columns in combined DataFrame: {missing_columns}\")\n",
        "\n",
        "    # Debugging: Combined DataFrame summary\n",
        "    print(\"\\nCombined DataFrame Summary:\")\n",
        "    print(f\"Shape: {combined_df.shape}\")\n",
        "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
        "    print(f\"Date Range: {combined_df.index.min()} to {combined_df.index.max()}\")\n",
        "\n",
        "    # Create df_2000 and df_2006\n",
        "    df_2000 = combined_df.reset_index()\n",
        "\n",
        "    # Filter for df_2006 (2006–present)\n",
        "    df_2006 = combined_df[combined_df.index >= '2006-01-01'].reset_index()\n",
        "\n",
        "    # Verify data quality\n",
        "    for df, name in [(df_2000, \"df_2000\"), (df_2006, \"df_2006\")]:\n",
        "        print(f\"\\n{name} Summary:\")\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "        print(f\"Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        duplicates = df[df['date'].duplicated(keep=False)]\n",
        "        if len(duplicates) > 0:\n",
        "            print(f\"Warning: {name} has {len(duplicates)} duplicate dates\")\n",
        "            print(duplicates.head())\n",
        "        # Check for NaN values\n",
        "        nan_counts = df[bls_columns].isna().sum()\n",
        "        if nan_counts.any():\n",
        "            print(f\"Warning: {name} has NaN values in BLS columns:\")\n",
        "            print(nan_counts[nan_counts > 0])\n",
        "\n",
        "    print(\"Data preprocessing completed successfully\")\n",
        "    return df_2000, df_2006\n",
        "\n",
        "df_2000, df_2006 = preprocess_data(start_year=2000)\n",
        "if df_2000 is not None and df_2006 is not None:\n",
        "    print(\"\\ndf_2000 Sample:\")\n",
        "    print(df_2000.head())\n",
        "    print(f\"df_2000 Columns: {df_2000.columns.tolist()}\")\n",
        "    print(\"\\ndf_2006 Sample:\")\n",
        "    print(df_2006.head())\n",
        "    print(f\"df_2006 Columns: {df_2006.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktzo8fVricN6",
        "outputId": "d8a5c67c-fca9-4aca-93f5-9edff2d2c503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FRED Data Summary:\n",
            "Shape: (1320, 12)\n",
            "Columns: ['date', 'Initial_Jobless_Claims', 'Total_Nonfarm_Employment', 'Unemployment_Rate', 'Manufacturing_Employment', 'Professional_Services_Employment', 'Leisure_Hospitality_Employment', 'Average_Hourly_Earnings', 'Weekly_Hours_Worked', 'GDP', 'Housing_Prices', 'Federal_Funds_Rate']\n",
            "Date Range: 2000-01-03 00:00:00 to 2025-04-14 00:00:00\n",
            "\n",
            "BLS Data Summary:\n",
            "Shape: (1256, 5)\n",
            "Columns: ['date', 'Total_Nonfarm_Job_Openings', 'Manufacturing_Job_Openings', 'Professional_Services_Job_Openings', 'Leisure_Hospitality_Job_Openings']\n",
            "Date Range: 2000-12-04 00:00:00 to 2025-02-24 00:00:00\n",
            "\n",
            "Merging FRED and BLS data from first overlapping date...\n",
            "\n",
            "Combined DataFrame Summary:\n",
            "Shape: (1256, 15)\n",
            "Columns: ['Initial_Jobless_Claims', 'Total_Nonfarm_Employment', 'Unemployment_Rate', 'Manufacturing_Employment', 'Professional_Services_Employment', 'Leisure_Hospitality_Employment', 'Average_Hourly_Earnings', 'Weekly_Hours_Worked', 'GDP', 'Housing_Prices', 'Federal_Funds_Rate', 'Total_Nonfarm_Job_Openings', 'Manufacturing_Job_Openings', 'Professional_Services_Job_Openings', 'Leisure_Hospitality_Job_Openings']\n",
            "Date Range: 2000-12-04 00:00:00 to 2025-02-24 00:00:00\n",
            "\n",
            "df_2000 Summary:\n",
            "Shape: (1256, 16)\n",
            "Date Range: 2000-12-04 00:00:00 to 2025-02-24 00:00:00\n",
            "\n",
            "df_2006 Summary:\n",
            "Shape: (991, 16)\n",
            "Date Range: 2006-01-02 00:00:00 to 2025-02-24 00:00:00\n",
            "Data preprocessing completed successfully\n",
            "\n",
            "df_2000 Sample:\n",
            "        date  Initial_Jobless_Claims  Total_Nonfarm_Employment  \\\n",
            "0 2000-12-04           333142.857143             132714.741935   \n",
            "1 2000-12-11           330428.571429             132711.806452   \n",
            "2 2000-12-18           356857.142857             132708.870968   \n",
            "3 2000-12-25           360857.142857             132705.935484   \n",
            "4 2001-01-01           348428.571429             132703.000000   \n",
            "\n",
            "   Unemployment_Rate  Manufacturing_Employment  \\\n",
            "0           3.929032              17173.645161   \n",
            "1           3.996774              17156.483871   \n",
            "2           4.064516              17139.322581   \n",
            "3           4.132258              17122.161290   \n",
            "4           4.200000              17105.000000   \n",
            "\n",
            "   Professional_Services_Employment  Leisure_Hospitality_Employment  \\\n",
            "0                      16893.483871                    11976.096774   \n",
            "1                      16894.612903                    11976.322581   \n",
            "2                      16895.741935                    11976.548387   \n",
            "3                      16896.870968                    11976.774194   \n",
            "4                      16898.000000                    11977.000000   \n",
            "\n",
            "   Average_Hourly_Earnings  Weekly_Hours_Worked           GDP  Housing_Prices  \\\n",
            "0                20.066129            34.216129  14190.348200      108.832935   \n",
            "1                20.066129            34.216129  14186.291302      108.928452   \n",
            "2                20.066129            34.216129  14183.513687      109.023968   \n",
            "3                20.066129            34.216129  14182.346280      109.119484   \n",
            "4                20.066129            34.216129  14183.120000      109.215000   \n",
            "\n",
            "   Federal_Funds_Rate  Total_Nonfarm_Job_Openings  Manufacturing_Job_Openings  \\\n",
            "0            6.359355                 5102.129032                  426.935484   \n",
            "1            6.264516                 5135.096774                  431.451613   \n",
            "2            6.169677                 5168.064516                  435.967742   \n",
            "3            6.074839                 5201.032258                  440.483871   \n",
            "4            5.980000                 5234.000000                  445.000000   \n",
            "\n",
            "   Professional_Services_Job_Openings  Leisure_Hospitality_Job_Openings  \n",
            "0                          911.548387                        574.516129  \n",
            "1                          894.161290                        594.387097  \n",
            "2                          876.774194                        614.258065  \n",
            "3                          859.387097                        634.129032  \n",
            "4                          842.000000                        654.000000  \n",
            "df_2000 Columns: ['date', 'Initial_Jobless_Claims', 'Total_Nonfarm_Employment', 'Unemployment_Rate', 'Manufacturing_Employment', 'Professional_Services_Employment', 'Leisure_Hospitality_Employment', 'Average_Hourly_Earnings', 'Weekly_Hours_Worked', 'GDP', 'Housing_Prices', 'Federal_Funds_Rate', 'Total_Nonfarm_Job_Openings', 'Manufacturing_Job_Openings', 'Professional_Services_Job_Openings', 'Leisure_Hospitality_Job_Openings']\n",
            "\n",
            "df_2006 Sample:\n",
            "        date  Initial_Jobless_Claims  Total_Nonfarm_Employment  \\\n",
            "0 2006-01-02           308857.142857             135435.838710   \n",
            "1 2006-01-09           314285.714286             135504.709677   \n",
            "2 2006-01-16           286428.571429             135573.580645   \n",
            "3 2006-01-23           287714.285714             135642.451613   \n",
            "4 2006-01-30           284000.000000             135711.322581   \n",
            "\n",
            "   Unemployment_Rate  Manufacturing_Employment  \\\n",
            "0           4.703226                   14208.0   \n",
            "1           4.725806                   14208.0   \n",
            "2           4.748387                   14208.0   \n",
            "3           4.770968                   14208.0   \n",
            "4           4.793548                   14208.0   \n",
            "\n",
            "   Professional_Services_Employment  Leisure_Hospitality_Employment  \\\n",
            "0                      17384.967742                    12946.129032   \n",
            "1                      17398.741935                    12954.032258   \n",
            "2                      17412.516129                    12961.935484   \n",
            "3                      17426.290323                    12969.838710   \n",
            "4                      17440.064516                    12977.741935   \n",
            "\n",
            "   Average_Hourly_Earnings  Weekly_Hours_Worked           GDP  Housing_Prices  \\\n",
            "0                20.066129            34.216129  16355.509680      180.849677   \n",
            "1                20.066129            34.216129  16366.129536      181.001419   \n",
            "2                20.066129            34.216129  16374.929752      181.153161   \n",
            "3                20.066129            34.216129  16382.064954      181.304903   \n",
            "4                20.066129            34.216129  16387.689771      181.456645   \n",
            "\n",
            "   Federal_Funds_Rate  Total_Nonfarm_Job_Openings  Manufacturing_Job_Openings  \\\n",
            "0            4.296452                 4394.677419                  320.129032   \n",
            "1            4.341613                 4378.419355                  328.032258   \n",
            "2            4.386774                 4362.161290                  335.935484   \n",
            "3            4.431935                 4345.903226                  343.838710   \n",
            "4            4.477097                 4329.645161                  351.741935   \n",
            "\n",
            "   Professional_Services_Job_Openings  Leisure_Hospitality_Job_Openings  \n",
            "0                          735.354839                        632.709677  \n",
            "1                          737.838710                        623.677419  \n",
            "2                          740.322581                        614.645161  \n",
            "3                          742.806452                        605.612903  \n",
            "4                          745.290323                        596.580645  \n",
            "df_2006 Columns: ['date', 'Initial_Jobless_Claims', 'Total_Nonfarm_Employment', 'Unemployment_Rate', 'Manufacturing_Employment', 'Professional_Services_Employment', 'Leisure_Hospitality_Employment', 'Average_Hourly_Earnings', 'Weekly_Hours_Worked', 'GDP', 'Housing_Prices', 'Federal_Funds_Rate', 'Total_Nonfarm_Job_Openings', 'Manufacturing_Job_Openings', 'Professional_Services_Job_Openings', 'Leisure_Hospitality_Job_Openings']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimised VAR**"
      ],
      "metadata": {
        "id": "5IOgwMwpcjGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Optimized Vector Autoregression (VAR) Pipeline (From Original) ===\n",
        "# - Retained original logic but added: NumPy vectorization where possible\n",
        "# - Enabled parallel processing for seasonal/maxlags grid search using joblib\n",
        "# - Used array-based interpolation and efficient differencing\n",
        "# - Inline comments indicate original vs optimized behavior\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.tsatools import detrend\n",
        "from joblib import Parallel, delayed\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Check Stationarity with ADF ===\n",
        "def check_stationarity(series, name, threshold=0.05):\n",
        "    try:\n",
        "        detrended = detrend(series.dropna(), order=1)\n",
        "        result = adfuller(detrended)\n",
        "        p_value = result[1]\n",
        "        return p_value < threshold\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# === Preprocessing for Log Transform and Noise Injection ===\n",
        "def preprocess_series(df):\n",
        "    transformed_df = df.copy()\n",
        "    log_columns = []\n",
        "    for col in transformed_df.columns:\n",
        "        std_val = transformed_df[col].std()\n",
        "        if std_val == 0:\n",
        "            transformed_df[col] += np.random.normal(0, 1e-6, len(transformed_df[col]))\n",
        "        if std_val > 5000 and (transformed_df[col] > 0).all():\n",
        "            transformed_df[col] = np.log(transformed_df[col])\n",
        "            log_columns.append(col)\n",
        "    return transformed_df, log_columns\n",
        "\n",
        "# === Make Series Stationary ===\n",
        "def make_stationary(df, threshold=0.05, seasonal_period=0):\n",
        "    diff_df = df.copy()\n",
        "    diff_columns = []\n",
        "    seasonal_diff_columns = []\n",
        "\n",
        "    for col in diff_df.columns:\n",
        "        if seasonal_period > 0 and len(diff_df[col]) > seasonal_period:\n",
        "            diff_df[col] = diff_df[col].diff(seasonal_period)\n",
        "            seasonal_diff_columns.append(col)\n",
        "        if not check_stationarity(diff_df[col].dropna(), col, threshold):\n",
        "            diff_df[col] = diff_df[col].diff()\n",
        "            diff_columns.append(col)\n",
        "\n",
        "    return diff_df.dropna(), diff_columns, seasonal_diff_columns\n",
        "\n",
        "# === Reverse Transformation to Original Scale ===\n",
        "def inverse_transform(forecast, original_df, test_start_idx, diff_columns, seasonal_diff_columns, log_columns, seasonal_period=0):\n",
        "    result = forecast.copy()\n",
        "\n",
        "    if seasonal_period > 0:\n",
        "        for col in seasonal_diff_columns:\n",
        "            if col in result.columns:\n",
        "                seasonal_start_idx = max(0, test_start_idx - seasonal_period)\n",
        "                base = original_df[col].iloc[seasonal_start_idx:test_start_idx].to_numpy()\n",
        "                seasonal_vals = np.zeros(result.shape[0])\n",
        "                for i in range(len(result)):\n",
        "                    seasonal_vals[i] = base[i % seasonal_period] + result[col].iloc[:i+1].sum()\n",
        "                result[col] = seasonal_vals\n",
        "\n",
        "    for col in diff_columns:\n",
        "        if col in result.columns:\n",
        "            result[col] = original_df[col].iloc[test_start_idx-1] + forecast[col].cumsum()\n",
        "\n",
        "    for col in log_columns:\n",
        "        if col in result.columns:\n",
        "            result[col] = np.exp(result[col])\n",
        "\n",
        "    return result\n",
        "\n",
        "# === Run Rolling Forecasts ===\n",
        "def run_var_model(df_stationary, df, df_no_date, forecast_horizon, window_size, maxlags, seasonal_period, diff_columns, seasonal_diff_columns, log_columns):\n",
        "    start_time = time.time()\n",
        "    initial_window = int(len(df_stationary) * window_size)\n",
        "    step_size = forecast_horizon\n",
        "    forecasts = []\n",
        "    actuals = []\n",
        "    forecast_dates = []\n",
        "\n",
        "    for start in range(initial_window, len(df_stationary), step_size):\n",
        "        train_end = start\n",
        "        test_start = start\n",
        "        test_end = min(start + forecast_horizon, len(df_stationary))\n",
        "\n",
        "        train = df_stationary.iloc[:train_end]\n",
        "        test = df_stationary.iloc[test_start:test_end]\n",
        "\n",
        "        try:\n",
        "            model = VAR(train)\n",
        "            lag_order = model.select_order(maxlags=maxlags).aic\n",
        "            results = model.fit(lag_order)\n",
        "        except ValueError:\n",
        "            model = VAR(train)\n",
        "            results = model.fit(1)\n",
        "            lag_order = 1\n",
        "\n",
        "        forecast_steps = len(test)\n",
        "        forecast = results.forecast(train.values[-lag_order:], steps=forecast_steps)\n",
        "        forecast_df = pd.DataFrame(forecast, index=test.index, columns=train.columns)\n",
        "\n",
        "        # === FIXED: Previously used df.index.get_loc(df['date'].iloc[train_end]) which caused KeyError ===\n",
        "        # === Now uses train_end directly as index position ===\n",
        "        test_start_idx = train_end\n",
        "\n",
        "        forecast_levels = inverse_transform(forecast_df, df_no_date, test_start_idx, diff_columns, seasonal_diff_columns, log_columns, seasonal_period)\n",
        "        test_levels = inverse_transform(test, df_no_date, test_start_idx, diff_columns, seasonal_diff_columns, log_columns, seasonal_period)\n",
        "\n",
        "        forecasts.append(forecast_levels)\n",
        "        actuals.append(test_levels)\n",
        "        forecast_dates.extend(df['date'].iloc[test_start:test_end])\n",
        "\n",
        "    forecast_all = pd.concat(forecasts)\n",
        "    actual_all = pd.concat(actuals)\n",
        "    mae = mean_absolute_error(actual_all['Total_Nonfarm_Job_Openings'], forecast_all['Total_Nonfarm_Job_Openings'])\n",
        "    runtime = time.time() - start_time\n",
        "\n",
        "    return mae, runtime, forecast_all, actual_all, forecast_dates\n",
        "\n",
        "# === Main VAR Grid Search (Now Parallelized) ===\n",
        "def simple_var_model(df, forecast_horizon=4, window_size=0.8):\n",
        "    print(f\"\\nInput df_2000: {len(df)} rows, {len(df.columns)} columns\")\n",
        "    if 'date' not in df.columns:\n",
        "        return None, None, None\n",
        "\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df_no_date = df.drop(columns=['date'])\n",
        "    df_transformed, log_columns = preprocess_series(df_no_date)\n",
        "\n",
        "    seasonal_periods = [0, 13, 26, 52]\n",
        "    maxlags_values = [2, 4, 6, 8]\n",
        "    key_vars = ['Total_Nonfarm_Job_Openings', 'Manufacturing_Job_Openings',\n",
        "                'Professional_Services_Job_Openings', 'Leisure_Hospitality_Job_Openings']\n",
        "\n",
        "    def run_config(seasonal_period, maxlags):\n",
        "        df_stationary, diff_columns, seasonal_diff_columns = make_stationary(df_transformed, threshold=0.05, seasonal_period=seasonal_period)\n",
        "        if df_stationary.empty or len(df_stationary.columns) < 2:\n",
        "            return None\n",
        "        return run_var_model(\n",
        "            df_stationary, df, df_no_date, forecast_horizon, window_size, maxlags,\n",
        "            seasonal_period, diff_columns, seasonal_diff_columns, log_columns\n",
        "        ) + (seasonal_period, maxlags)\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(run_config)(sp, ml)\n",
        "        for sp in seasonal_periods for ml in maxlags_values\n",
        "    )\n",
        "\n",
        "    results = [r for r in results if r is not None]\n",
        "    best_result = min(results, key=lambda x: x[0])\n",
        "\n",
        "    forecast_all = best_result[2]\n",
        "    actual_all = best_result[3]\n",
        "    forecast_dates = best_result[4]\n",
        "\n",
        "    for var in key_vars:\n",
        "        if var in actual_all.columns:\n",
        "            mae = mean_absolute_error(actual_all[var], forecast_all[var])\n",
        "            rmse = np.sqrt(mean_squared_error(actual_all[var], forecast_all[var]))\n",
        "            print(f\"{var} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, var in enumerate(key_vars, 1):\n",
        "        if var in actual_all.columns:\n",
        "            plt.subplot(2, 2, i)\n",
        "            plt.plot(forecast_dates, actual_all[var], label='Actual')\n",
        "            plt.plot(forecast_dates, forecast_all[var], label='Forecast')\n",
        "            plt.title(f\"{var} (Levels)\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('forecast_vs_actual.png')\n",
        "    plt.close()\n",
        "    print(\"\\nPlot saved as 'forecast_vs_actual.png'\")\n",
        "\n",
        "    return best_result[5], best_result[6], forecast_all\n",
        "\n",
        "# === Execution Guard ===\n",
        "if df_2000 is not None:\n",
        "    best_seasonal_period, best_maxlags, forecast_df = simple_var_model(df_2000, forecast_horizon=4, window_size=0.8)\n",
        "    if forecast_df is not None:\n",
        "        print(f\"\\nBest parameters: seasonal_period={best_seasonal_period}, maxlags={best_maxlags}\")\n",
        "        print(\"\\nForecast Sample (First 5 Rows, level values):\")\n",
        "        print(forecast_df.head())\n",
        "else:\n",
        "    print(\"Error: Failed to load df_2000\")\n",
        "\n",
        "#Original: 1m 56s\n",
        "#Optimised: 1m 5s\n",
        "\n",
        "#Speedup ≈ 43.97%\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEWw4j-8cikO",
        "outputId": "a0c11df0-7356-48da-f566-6611aa93e3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input df_2000: 1256 rows, 16 columns\n",
            "Total_Nonfarm_Job_Openings - MAE: 114.28, RMSE: 175.43\n",
            "Manufacturing_Job_Openings - MAE: 15.65, RMSE: 24.10\n",
            "Professional_Services_Job_Openings - MAE: 28.22, RMSE: 40.87\n",
            "Leisure_Hospitality_Job_Openings - MAE: 30.26, RMSE: 48.48\n",
            "\n",
            "Plot saved as 'forecast_vs_actual.png'\n",
            "\n",
            "Best parameters: seasonal_period=0, maxlags=6\n",
            "\n",
            "Forecast Sample (First 5 Rows, level values):\n",
            "      Initial_Jobless_Claims  Total_Nonfarm_Employment  Unemployment_Rate  \\\n",
            "1005                     inf                       inf           8.714744   \n",
            "1006                     inf                       inf           7.308343   \n",
            "1007                     inf                       inf           6.631081   \n",
            "1008                     inf                       inf           6.095992   \n",
            "1009                     inf                       inf           9.832289   \n",
            "\n",
            "      Manufacturing_Employment  Professional_Services_Employment  \\\n",
            "1005              11679.883028                      19495.884968   \n",
            "1006              11744.956515                      19552.941351   \n",
            "1007              11845.433467                      19678.885118   \n",
            "1008              11943.816744                      19826.938928   \n",
            "1009              11932.431739                      19750.874185   \n",
            "\n",
            "      Leisure_Hospitality_Employment  Average_Hourly_Earnings  \\\n",
            "1005                    10752.087782                29.628581   \n",
            "1006                    11212.296685                29.512067   \n",
            "1007                    11848.345479                29.372588   \n",
            "1008                    12504.114229                29.236995   \n",
            "1009                    12112.612082                29.392157   \n",
            "\n",
            "      Weekly_Hours_Worked           GDP  Housing_Prices  Federal_Funds_Rate  \\\n",
            "1005            34.640839  19427.078147      218.227221            0.200347   \n",
            "1006            34.603849  19569.453093      218.333888            0.421169   \n",
            "1007            34.550705  19687.838661      218.433923            0.618056   \n",
            "1008            34.511814  19775.405822      218.452433            0.819925   \n",
            "1009            34.631191  19954.557046      219.802300            0.097811   \n",
            "\n",
            "      Total_Nonfarm_Job_Openings  Manufacturing_Job_Openings  \\\n",
            "1005                 5719.091213                  317.017050   \n",
            "1006                 5866.106237                  335.084232   \n",
            "1007                 5987.648630                  361.527217   \n",
            "1008                 6129.057736                  389.513922   \n",
            "1009                 6164.237383                  339.544154   \n",
            "\n",
            "      Professional_Services_Job_Openings  Leisure_Hospitality_Job_Openings  \n",
            "1005                          975.915302                        750.696549  \n",
            "1006                          906.561404                        819.571823  \n",
            "1007                          894.068795                        886.735972  \n",
            "1008                          901.754998                        964.080687  \n",
            "1009                         1120.486526                        881.228170  \n"
          ]
        }
      ]
    }
  ]
}